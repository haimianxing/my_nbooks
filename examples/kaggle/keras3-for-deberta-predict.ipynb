{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T05:35:05.554556Z",
     "iopub.status.busy": "2024-05-17T05:35:05.554001Z",
     "iopub.status.idle": "2024-05-17T05:35:07.148316Z",
     "shell.execute_reply": "2024-05-17T05:35:07.147191Z",
     "shell.execute_reply.started": "2024-05-17T05:35:05.554526Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pii-data-detection-ckpt-ds/submission.csv\n",
      "/kaggle/input/pii-data-detection-ckpt-ds/model.weights.h5\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/config.json\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/tokenizer.json\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/metadata.json\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/model.weights.h5\n",
      "/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/assets/tokenizer/vocabulary.spm\n",
      "/kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/kerasv3-lib-ds/keras_cv-0.8.1-py3-none-any.whl\n",
      "/kaggle/input/kerasv3-lib-ds/keras_cv-0.8.2-py3-none-any.whl\n",
      "/kaggle/input/kerasv3-lib-ds/keras-3.0.2-py3-none-any.whl\n",
      "/kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl\n",
      "/kaggle/input/kerasv3-lib-ds/tensorflow_io-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/train.json\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/test.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:35:07.150293Z",
     "iopub.status.busy": "2024-05-17T05:35:07.149926Z",
     "iopub.status.idle": "2024-05-17T05:36:37.238682Z",
     "shell.execute_reply": "2024-05-17T05:36:37.237547Z",
     "shell.execute_reply.started": "2024-05-17T05:35:07.150267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (1.51.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0.post1) (2.15.0)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0.post1)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0.post1) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (2.26.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (3.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.0.post1) (3.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0.post1) (3.2.2)\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.2.1\n",
      "    Uninstalling keras-3.2.1:\n",
      "      Successfully uninstalled keras-3.2.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 tensorflow-2.15.0.post1\n",
      "Processing /kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras==3.0.4) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras==3.0.4) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras==3.0.4) (13.7.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras==3.0.4) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras==3.0.4) (3.10.0)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras==3.0.4) (0.1.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras==3.0.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras==3.0.4) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras==3.0.4) (0.1.2)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "tensorflow 2.15.0.post1 requires keras<2.16,>=2.15.0, but you have keras 3.0.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install /kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:37.240564Z",
     "iopub.status.busy": "2024-05-17T05:36:37.240223Z",
     "iopub.status.idle": "2024-05-17T05:36:48.383380Z",
     "shell.execute_reply": "2024-05-17T05:36:48.382328Z",
     "shell.execute_reply.started": "2024-05-17T05:36:37.240534Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 05:36:40.533401: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-17 05:36:40.533464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-17 05:36:40.534892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras \n",
    "import keras_nlp\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:48.385231Z",
     "iopub.status.busy": "2024-05-17T05:36:48.384676Z",
     "iopub.status.idle": "2024-05-17T05:36:48.390564Z",
     "shell.execute_reply": "2024-05-17T05:36:48.389464Z",
     "shell.execute_reply.started": "2024-05-17T05:36:48.385204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf 2.15.0\n",
      "ks 3.0.4\n",
      "ks_np 0.9.3\n"
     ]
    }
   ],
   "source": [
    "print('tf',tf.__version__)\n",
    "print('ks',keras.__version__)\n",
    "print('ks_np',keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:48.393274Z",
     "iopub.status.busy": "2024-05-17T05:36:48.392960Z",
     "iopub.status.idle": "2024-05-17T05:36:48.406765Z",
     "shell.execute_reply": "2024-05-17T05:36:48.405925Z",
     "shell.execute_reply.started": "2024-05-17T05:36:48.393251Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    preset = \"deberta_v3_small_en\" # name of pretrained backbone\n",
    "    train_seq_len = 1024 # max size of input sequence for training\n",
    "    train_batch_size = 2 * 8 # size of the input batch in training, x 2 as two GPUs\n",
    "    infer_seq_len = 2000 # max size of input sequence for inference\n",
    "    infer_batch_size = 2 * 2 # size of the input batch in inference, x 2 as two GPUs\n",
    "    epochs = 6 # number of epochs to train\n",
    "    lr_mode = \"exp\" # lr scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
    "    \n",
    "    labels = [\"B-EMAIL\", \"B-ID_NUM\", \"B-NAME_STUDENT\", \"B-PHONE_NUM\",\n",
    "              \"B-STREET_ADDRESS\", \"B-URL_PERSONAL\", \"B-USERNAME\",\n",
    "              \"I-ID_NUM\", \"I-NAME_STUDENT\", \"I-PHONE_NUM\",\n",
    "              \"I-STREET_ADDRESS\",\"I-URL_PERSONAL\",\"O\"]\n",
    "    id2label = dict(enumerate(labels)) # integer label to BIO format label mapping\n",
    "    label2id = {v:k for k,v in id2label.items()} # BIO format label to integer label mapping\n",
    "    num_labels = len(labels) # number of PII (NER) tags\n",
    "    \n",
    "    train = True # whether to train or use already trained ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:48.407957Z",
     "iopub.status.busy": "2024-05-17T05:36:48.407719Z",
     "iopub.status.idle": "2024-05-17T05:36:48.424103Z",
     "shell.execute_reply": "2024-05-17T05:36:48.423416Z",
     "shell.execute_reply.started": "2024-05-17T05:36:48.407936Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:48.425454Z",
     "iopub.status.busy": "2024-05-17T05:36:48.425162Z",
     "iopub.status.idle": "2024-05-17T05:36:49.259102Z",
     "shell.execute_reply": "2024-05-17T05:36:49.258081Z",
     "shell.execute_reply.started": "2024-05-17T05:36:48.425431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpu:0', 'gpu:1']\n"
     ]
    }
   ],
   "source": [
    "devices = keras.distribution.list_devices() #获取设备列表\n",
    "print(devices)\n",
    "\n",
    "if len(devices)>1:\n",
    "    data_parallel = keras.distribution.DataParallel(devices=devices) #根据gpu数计算数据并行数量\n",
    "    keras.distribution.set_distribution(data_parallel) #设置数据并行数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:49.260622Z",
     "iopub.status.busy": "2024-05-17T05:36:49.260310Z",
     "iopub.status.idle": "2024-05-17T05:36:49.265170Z",
     "shell.execute_reply": "2024-05-17T05:36:49.264206Z",
     "shell.execute_reply.started": "2024-05-17T05:36:49.260596Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:49.266622Z",
     "iopub.status.busy": "2024-05-17T05:36:49.266303Z",
     "iopub.status.idle": "2024-05-17T05:36:49.278604Z",
     "shell.execute_reply": "2024-05-17T05:36:49.277740Z",
     "shell.execute_reply.started": "2024-05-17T05:36:49.266595Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"/kaggle/input/pii-detection-removal-from-educational-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:49.280468Z",
     "iopub.status.busy": "2024-05-17T05:36:49.279821Z",
     "iopub.status.idle": "2024-05-17T05:36:52.188070Z",
     "shell.execute_reply": "2024-05-17T05:36:52.187243Z",
     "shell.execute_reply.started": "2024-05-17T05:36:49.280435Z"
    }
   },
   "outputs": [],
   "source": [
    "data = json.load(open(f'{BASE_PATH}/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:52.189415Z",
     "iopub.status.busy": "2024-05-17T05:36:52.189135Z",
     "iopub.status.idle": "2024-05-17T05:36:52.236165Z",
     "shell.execute_reply": "2024-05-17T05:36:52.235121Z",
     "shell.execute_reply.started": "2024-05-17T05:36:52.189383Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 7,\n",
       " 'full_text': \"Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nChallenge & selection\\n\\nThe tool I use to help all stakeholders finding their way through the complexity of a project is the  mind map.\\n\\nWhat exactly is a mind map? According to the definition of Buzan T. and Buzan B. (1999, Dessine-moi  l'intelligence. Paris: Les Éditions d'Organisation.), the mind map (or heuristic diagram) is a graphic  representation technique that follows the natural functioning of the mind and allows the brain's  potential to be released. Cf Annex1\\n\\nThis tool has many advantages:\\n\\n•  It is accessible to all and does not require significant material investment and can be done  quickly\\n\\n•  It is scalable\\n\\n•  It allows categorization and linking of information\\n\\n•  It can be applied to any type of situation: notetaking, problem solving, analysis, creation of  new ideas\\n\\n•  It is suitable for all people and is easy to learn\\n\\n•  It is fun and encourages exchanges\\n\\n•  It makes visible the dimension of projects, opportunities, interconnections\\n\\n•  It synthesizes\\n\\n•  It makes the project understandable\\n\\n•  It allows you to explore ideas\\n\\nThe creation of a mind map starts with an idea/problem located at its center. This starting point  generates ideas/work areas, incremented around this center in a radial structure, which in turn is  completed with as many branches as new ideas.\\n\\nThis tool enables creativity and logic to be mobilized, it is a map of the thoughts.\\n\\nCreativity is enhanced because participants feel comfortable with the method.\\n\\nApplication & Insight\\n\\nI start the process of the mind map creation with the stakeholders standing around a large board  (white or paper board). In the center of the board, I write and highlight the topic to design.\\n\\nThrough a series of questions, I guide the stakeholders in modelling the mind map. I adapt the series  of questions according to the topic to be addressed. In the type of questions, we can use: who, what,  when, where, why, how, how much.\\n\\nThe use of the “why” is very interesting to understand the origin. By this way, the interviewed person  frees itself from paradigms and thus dares to propose new ideas / ways of functioning. I plan two  hours for a workshop.\\n\\nDesign Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nAfter modelling the mind map on paper, I propose to the participants a digital visualization of their  work with the addition of color codes, images and interconnections. This second workshop also lasts  two hours and allows the mind map to evolve. Once familiarized with it, the stakeholders discover  the power of the tool. Then, the second workshop brings out even more ideas and constructive  exchanges between the stakeholders. Around this new mind map, they have learned to work  together and want to make visible the untold ideas.\\n\\nI now present all the projects I manage in this type of format in order to ease rapid understanding for  decision-makers. These presentations are the core of my business models. The decision-makers are  thus able to identify the opportunities of the projects and can take quick decisions to validate them.  They find answers to their questions thank to a schematic representation.\\n\\nApproach\\n\\nWhat I find amazing with the facilitation of this type of workshop is the participants commitment for  the project. This tool helps to give meaning. The participants appropriate the story and want to keep  writing it. Then, they easily become actors or sponsors of the project. A trust relationship is built,  thus facilitating the implementation of related actions.\\n\\nDesign Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nAnnex 1: Mind Map Shared facilities project\\n\\n\",\n",
       " 'tokens': ['Design',\n",
       "  'Thinking',\n",
       "  'for',\n",
       "  'innovation',\n",
       "  'reflexion',\n",
       "  '-',\n",
       "  'Avril',\n",
       "  '2021',\n",
       "  '-',\n",
       "  'Nathalie',\n",
       "  'Sylla',\n",
       "  '\\n\\n',\n",
       "  'Challenge',\n",
       "  '&',\n",
       "  'selection',\n",
       "  '\\n\\n',\n",
       "  'The',\n",
       "  'tool',\n",
       "  'I',\n",
       "  'use',\n",
       "  'to',\n",
       "  'help',\n",
       "  'all',\n",
       "  'stakeholders',\n",
       "  'finding',\n",
       "  'their',\n",
       "  'way',\n",
       "  'through',\n",
       "  'the',\n",
       "  'complexity',\n",
       "  'of',\n",
       "  'a',\n",
       "  'project',\n",
       "  'is',\n",
       "  'the',\n",
       "  ' ',\n",
       "  'mind',\n",
       "  'map',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'What',\n",
       "  'exactly',\n",
       "  'is',\n",
       "  'a',\n",
       "  'mind',\n",
       "  'map',\n",
       "  '?',\n",
       "  'According',\n",
       "  'to',\n",
       "  'the',\n",
       "  'definition',\n",
       "  'of',\n",
       "  'Buzan',\n",
       "  'T.',\n",
       "  'and',\n",
       "  'Buzan',\n",
       "  'B.',\n",
       "  '(',\n",
       "  '1999',\n",
       "  ',',\n",
       "  'Dessine',\n",
       "  '-',\n",
       "  'moi',\n",
       "  ' ',\n",
       "  \"l'intelligence\",\n",
       "  '.',\n",
       "  'Paris',\n",
       "  ':',\n",
       "  'Les',\n",
       "  'Éditions',\n",
       "  \"d'Organisation\",\n",
       "  '.',\n",
       "  ')',\n",
       "  ',',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'map',\n",
       "  '(',\n",
       "  'or',\n",
       "  'heuristic',\n",
       "  'diagram',\n",
       "  ')',\n",
       "  'is',\n",
       "  'a',\n",
       "  'graphic',\n",
       "  ' ',\n",
       "  'representation',\n",
       "  'technique',\n",
       "  'that',\n",
       "  'follows',\n",
       "  'the',\n",
       "  'natural',\n",
       "  'functioning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'and',\n",
       "  'allows',\n",
       "  'the',\n",
       "  'brain',\n",
       "  \"'s\",\n",
       "  ' ',\n",
       "  'potential',\n",
       "  'to',\n",
       "  'be',\n",
       "  'released',\n",
       "  '.',\n",
       "  'Cf',\n",
       "  'Annex1',\n",
       "  '\\n\\n',\n",
       "  'This',\n",
       "  'tool',\n",
       "  'has',\n",
       "  'many',\n",
       "  'advantages',\n",
       "  ':',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'is',\n",
       "  'accessible',\n",
       "  'to',\n",
       "  'all',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'require',\n",
       "  'significant',\n",
       "  'material',\n",
       "  'investment',\n",
       "  'and',\n",
       "  'can',\n",
       "  'be',\n",
       "  'done',\n",
       "  ' ',\n",
       "  'quickly',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'is',\n",
       "  'scalable',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'allows',\n",
       "  'categorization',\n",
       "  'and',\n",
       "  'linking',\n",
       "  'of',\n",
       "  'information',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'can',\n",
       "  'be',\n",
       "  'applied',\n",
       "  'to',\n",
       "  'any',\n",
       "  'type',\n",
       "  'of',\n",
       "  'situation',\n",
       "  ':',\n",
       "  'notetaking',\n",
       "  ',',\n",
       "  'problem',\n",
       "  'solving',\n",
       "  ',',\n",
       "  'analysis',\n",
       "  ',',\n",
       "  'creation',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'new',\n",
       "  'ideas',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'is',\n",
       "  'suitable',\n",
       "  'for',\n",
       "  'all',\n",
       "  'people',\n",
       "  'and',\n",
       "  'is',\n",
       "  'easy',\n",
       "  'to',\n",
       "  'learn',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'is',\n",
       "  'fun',\n",
       "  'and',\n",
       "  'encourages',\n",
       "  'exchanges',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'makes',\n",
       "  'visible',\n",
       "  'the',\n",
       "  'dimension',\n",
       "  'of',\n",
       "  'projects',\n",
       "  ',',\n",
       "  'opportunities',\n",
       "  ',',\n",
       "  'interconnections',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'synthesizes',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'makes',\n",
       "  'the',\n",
       "  'project',\n",
       "  'understandable',\n",
       "  '\\n\\n',\n",
       "  '•',\n",
       "  ' ',\n",
       "  'It',\n",
       "  'allows',\n",
       "  'you',\n",
       "  'to',\n",
       "  'explore',\n",
       "  'ideas',\n",
       "  '\\n\\n',\n",
       "  'The',\n",
       "  'creation',\n",
       "  'of',\n",
       "  'a',\n",
       "  'mind',\n",
       "  'map',\n",
       "  'starts',\n",
       "  'with',\n",
       "  'an',\n",
       "  'idea',\n",
       "  '/',\n",
       "  'problem',\n",
       "  'located',\n",
       "  'at',\n",
       "  'its',\n",
       "  'center',\n",
       "  '.',\n",
       "  'This',\n",
       "  'starting',\n",
       "  'point',\n",
       "  ' ',\n",
       "  'generates',\n",
       "  'ideas',\n",
       "  '/',\n",
       "  'work',\n",
       "  'areas',\n",
       "  ',',\n",
       "  'incremented',\n",
       "  'around',\n",
       "  'this',\n",
       "  'center',\n",
       "  'in',\n",
       "  'a',\n",
       "  'radial',\n",
       "  'structure',\n",
       "  ',',\n",
       "  'which',\n",
       "  'in',\n",
       "  'turn',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'completed',\n",
       "  'with',\n",
       "  'as',\n",
       "  'many',\n",
       "  'branches',\n",
       "  'as',\n",
       "  'new',\n",
       "  'ideas',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'This',\n",
       "  'tool',\n",
       "  'enables',\n",
       "  'creativity',\n",
       "  'and',\n",
       "  'logic',\n",
       "  'to',\n",
       "  'be',\n",
       "  'mobilized',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'map',\n",
       "  'of',\n",
       "  'the',\n",
       "  'thoughts',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'Creativity',\n",
       "  'is',\n",
       "  'enhanced',\n",
       "  'because',\n",
       "  'participants',\n",
       "  'feel',\n",
       "  'comfortable',\n",
       "  'with',\n",
       "  'the',\n",
       "  'method',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'Application',\n",
       "  '&',\n",
       "  'Insight',\n",
       "  '\\n\\n',\n",
       "  'I',\n",
       "  'start',\n",
       "  'the',\n",
       "  'process',\n",
       "  'of',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'map',\n",
       "  'creation',\n",
       "  'with',\n",
       "  'the',\n",
       "  'stakeholders',\n",
       "  'standing',\n",
       "  'around',\n",
       "  'a',\n",
       "  'large',\n",
       "  'board',\n",
       "  ' ',\n",
       "  '(',\n",
       "  'white',\n",
       "  'or',\n",
       "  'paper',\n",
       "  'board',\n",
       "  ')',\n",
       "  '.',\n",
       "  'In',\n",
       "  'the',\n",
       "  'center',\n",
       "  'of',\n",
       "  'the',\n",
       "  'board',\n",
       "  ',',\n",
       "  'I',\n",
       "  'write',\n",
       "  'and',\n",
       "  'highlight',\n",
       "  'the',\n",
       "  'topic',\n",
       "  'to',\n",
       "  'design',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'Through',\n",
       "  'a',\n",
       "  'series',\n",
       "  'of',\n",
       "  'questions',\n",
       "  ',',\n",
       "  'I',\n",
       "  'guide',\n",
       "  'the',\n",
       "  'stakeholders',\n",
       "  'in',\n",
       "  'modelling',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'map',\n",
       "  '.',\n",
       "  'I',\n",
       "  'adapt',\n",
       "  'the',\n",
       "  'series',\n",
       "  ' ',\n",
       "  'of',\n",
       "  'questions',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'topic',\n",
       "  'to',\n",
       "  'be',\n",
       "  'addressed',\n",
       "  '.',\n",
       "  'In',\n",
       "  'the',\n",
       "  'type',\n",
       "  'of',\n",
       "  'questions',\n",
       "  ',',\n",
       "  'we',\n",
       "  'can',\n",
       "  'use',\n",
       "  ':',\n",
       "  'who',\n",
       "  ',',\n",
       "  'what',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'when',\n",
       "  ',',\n",
       "  'where',\n",
       "  ',',\n",
       "  'why',\n",
       "  ',',\n",
       "  'how',\n",
       "  ',',\n",
       "  'how',\n",
       "  'much',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'The',\n",
       "  'use',\n",
       "  'of',\n",
       "  'the',\n",
       "  '“',\n",
       "  'why',\n",
       "  '”',\n",
       "  'is',\n",
       "  'very',\n",
       "  'interesting',\n",
       "  'to',\n",
       "  'understand',\n",
       "  'the',\n",
       "  'origin',\n",
       "  '.',\n",
       "  'By',\n",
       "  'this',\n",
       "  'way',\n",
       "  ',',\n",
       "  'the',\n",
       "  'interviewed',\n",
       "  'person',\n",
       "  ' ',\n",
       "  'frees',\n",
       "  'itself',\n",
       "  'from',\n",
       "  'paradigms',\n",
       "  'and',\n",
       "  'thus',\n",
       "  'dares',\n",
       "  'to',\n",
       "  'propose',\n",
       "  'new',\n",
       "  'ideas',\n",
       "  '/',\n",
       "  'ways',\n",
       "  'of',\n",
       "  'functioning',\n",
       "  '.',\n",
       "  'I',\n",
       "  'plan',\n",
       "  'two',\n",
       "  ' ',\n",
       "  'hours',\n",
       "  'for',\n",
       "  'a',\n",
       "  'workshop',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'Design',\n",
       "  'Thinking',\n",
       "  'for',\n",
       "  'innovation',\n",
       "  'reflexion',\n",
       "  '-',\n",
       "  'Avril',\n",
       "  '2021',\n",
       "  '-',\n",
       "  'Nathalie',\n",
       "  'Sylla',\n",
       "  '\\n\\n',\n",
       "  'After',\n",
       "  'modelling',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'map',\n",
       "  'on',\n",
       "  'paper',\n",
       "  ',',\n",
       "  'I',\n",
       "  'propose',\n",
       "  'to',\n",
       "  'the',\n",
       "  'participants',\n",
       "  'a',\n",
       "  'digital',\n",
       "  'visualization',\n",
       "  'of',\n",
       "  'their',\n",
       "  ' ',\n",
       "  'work',\n",
       "  'with',\n",
       "  'the',\n",
       "  'addition',\n",
       "  'of',\n",
       "  'color',\n",
       "  'codes',\n",
       "  ',',\n",
       "  'images',\n",
       "  'and',\n",
       "  'interconnections',\n",
       "  '.',\n",
       "  'This',\n",
       "  'second',\n",
       "  'workshop',\n",
       "  'also',\n",
       "  'lasts',\n",
       "  ' ',\n",
       "  'two',\n",
       "  'hours',\n",
       "  'and',\n",
       "  'allows',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'map',\n",
       "  'to',\n",
       "  'evolve',\n",
       "  '.',\n",
       "  'Once',\n",
       "  'familiarized',\n",
       "  'with',\n",
       "  'it',\n",
       "  ',',\n",
       "  'the',\n",
       "  'stakeholders',\n",
       "  'discover',\n",
       "  ' ',\n",
       "  'the',\n",
       "  'power',\n",
       "  'of',\n",
       "  'the',\n",
       "  'tool',\n",
       "  '.',\n",
       "  'Then',\n",
       "  ',',\n",
       "  'the',\n",
       "  'second',\n",
       "  'workshop',\n",
       "  'brings',\n",
       "  'out',\n",
       "  'even',\n",
       "  'more',\n",
       "  'ideas',\n",
       "  'and',\n",
       "  'constructive',\n",
       "  ' ',\n",
       "  'exchanges',\n",
       "  'between',\n",
       "  'the',\n",
       "  'stakeholders',\n",
       "  '.',\n",
       "  'Around',\n",
       "  'this',\n",
       "  'new',\n",
       "  'mind',\n",
       "  'map',\n",
       "  ',',\n",
       "  'they',\n",
       "  'have',\n",
       "  'learned',\n",
       "  'to',\n",
       "  'work',\n",
       "  ' ',\n",
       "  'together',\n",
       "  'and',\n",
       "  'want',\n",
       "  'to',\n",
       "  'make',\n",
       "  'visible',\n",
       "  'the',\n",
       "  'untold',\n",
       "  'ideas',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'I',\n",
       "  'now',\n",
       "  'present',\n",
       "  'all',\n",
       "  'the',\n",
       "  'projects',\n",
       "  'I',\n",
       "  'manage',\n",
       "  'in',\n",
       "  'this',\n",
       "  'type',\n",
       "  'of',\n",
       "  'format',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'ease',\n",
       "  'rapid',\n",
       "  'understanding',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'decision',\n",
       "  '-',\n",
       "  'makers',\n",
       "  '.',\n",
       "  'These',\n",
       "  'presentations',\n",
       "  'are',\n",
       "  'the',\n",
       "  'core',\n",
       "  'of',\n",
       "  'my',\n",
       "  'business',\n",
       "  'models',\n",
       "  '.',\n",
       "  'The',\n",
       "  'decision',\n",
       "  '-',\n",
       "  'makers',\n",
       "  'are',\n",
       "  ' ',\n",
       "  'thus',\n",
       "  'able',\n",
       "  'to',\n",
       "  'identify',\n",
       "  'the',\n",
       "  'opportunities',\n",
       "  'of',\n",
       "  'the',\n",
       "  'projects',\n",
       "  'and',\n",
       "  'can',\n",
       "  'take',\n",
       "  'quick',\n",
       "  'decisions',\n",
       "  'to',\n",
       "  'validate',\n",
       "  'them',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'They',\n",
       "  'find',\n",
       "  'answers',\n",
       "  'to',\n",
       "  'their',\n",
       "  'questions',\n",
       "  'thank',\n",
       "  'to',\n",
       "  'a',\n",
       "  'schematic',\n",
       "  'representation',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'Approach',\n",
       "  '\\n\\n',\n",
       "  'What',\n",
       "  'I',\n",
       "  'find',\n",
       "  'amazing',\n",
       "  'with',\n",
       "  'the',\n",
       "  'facilitation',\n",
       "  'of',\n",
       "  'this',\n",
       "  'type',\n",
       "  'of',\n",
       "  'workshop',\n",
       "  'is',\n",
       "  'the',\n",
       "  'participants',\n",
       "  'commitment',\n",
       "  'for',\n",
       "  ' ',\n",
       "  'the',\n",
       "  'project',\n",
       "  '.',\n",
       "  'This',\n",
       "  'tool',\n",
       "  'helps',\n",
       "  'to',\n",
       "  'give',\n",
       "  'meaning',\n",
       "  '.',\n",
       "  'The',\n",
       "  'participants',\n",
       "  'appropriate',\n",
       "  'the',\n",
       "  'story',\n",
       "  'and',\n",
       "  'want',\n",
       "  'to',\n",
       "  'keep',\n",
       "  ' ',\n",
       "  'writing',\n",
       "  'it',\n",
       "  '.',\n",
       "  'Then',\n",
       "  ',',\n",
       "  'they',\n",
       "  'easily',\n",
       "  'become',\n",
       "  'actors',\n",
       "  'or',\n",
       "  'sponsors',\n",
       "  'of',\n",
       "  'the',\n",
       "  'project',\n",
       "  '.',\n",
       "  'A',\n",
       "  'trust',\n",
       "  'relationship',\n",
       "  'is',\n",
       "  'built',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'thus',\n",
       "  'facilitating',\n",
       "  'the',\n",
       "  'implementation',\n",
       "  'of',\n",
       "  'related',\n",
       "  'actions',\n",
       "  '.',\n",
       "  '\\n\\n',\n",
       "  'Design',\n",
       "  'Thinking',\n",
       "  'for',\n",
       "  'innovation',\n",
       "  'reflexion',\n",
       "  '-',\n",
       "  'Avril',\n",
       "  '2021',\n",
       "  '-',\n",
       "  'Nathalie',\n",
       "  'Sylla',\n",
       "  '\\n\\n',\n",
       "  'Annex',\n",
       "  '1',\n",
       "  ':',\n",
       "  'Mind',\n",
       "  'Map',\n",
       "  'Shared',\n",
       "  'facilities',\n",
       "  'project',\n",
       "  '\\n\\n'],\n",
       " 'trailing_whitespace': [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  False,\n",
       "  False],\n",
       " 'labels': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-NAME_STUDENT',\n",
       "  'I-NAME_STUDENT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-NAME_STUDENT',\n",
       "  'I-NAME_STUDENT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-NAME_STUDENT',\n",
       "  'I-NAME_STUDENT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:52.237662Z",
     "iopub.status.busy": "2024-05-17T05:36:52.237397Z",
     "iopub.status.idle": "2024-05-17T05:36:52.257714Z",
     "shell.execute_reply": "2024-05-17T05:36:52.256828Z",
     "shell.execute_reply.started": "2024-05-17T05:36:52.237639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, ..., None, None, None], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.empty(len(data),dtype=object)\n",
    "labels = np.empty(len(data),dtype=object)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:52.259012Z",
     "iopub.status.busy": "2024-05-17T05:36:52.258724Z",
     "iopub.status.idle": "2024-05-17T05:36:55.034356Z",
     "shell.execute_reply": "2024-05-17T05:36:55.033413Z",
     "shell.execute_reply.started": "2024-05-17T05:36:52.258980Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1156e62756e434facea97e9a2a40611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([array([12, 12, 12, 12, 12, 12, 12, 12, 12,  2,  8, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12,  2,  8, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  2,  8, 12, 12, 12, 12, 12,\n",
       "              12, 12, 12, 12, 12])                                               ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,v in tqdm(enumerate(data),total=len(data)):\n",
    "    words[i] = np.array(v[\"tokens\"])\n",
    "    labels[i] = np.array([CFG.label2id[label] for label in v[\"labels\"]])\n",
    "labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:55.038900Z",
     "iopub.status.busy": "2024-05-17T05:36:55.038607Z",
     "iopub.status.idle": "2024-05-17T05:36:55.044691Z",
     "shell.execute_reply": "2024-05-17T05:36:55.043512Z",
     "shell.execute_reply.started": "2024-05-17T05:36:55.038875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:55.046020Z",
     "iopub.status.busy": "2024-05-17T05:36:55.045781Z",
     "iopub.status.idle": "2024-05-17T05:36:56.537852Z",
     "shell.execute_reply": "2024-05-17T05:36:56.536924Z",
     "shell.execute_reply.started": "2024-05-17T05:36:55.045999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"d892c907-5551-473e-a753-0a7fef61a32b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d892c907-5551-473e-a753-0a7fef61a32b\")) {                    Plotly.newPlot(                        \"d892c907-5551-473e-a753-0a7fef61a32b\",                        [{\"x\":[\"B-EMAIL\",\"B-ID_NUM\",\"B-NAME_STUDENT\",\"B-PHONE_NUM\",\"B-STREET_ADDRESS\",\"B-URL_PERSONAL\",\"B-USERNAME\",\"I-ID_NUM\",\"I-NAME_STUDENT\",\"I-PHONE_NUM\",\"I-STREET_ADDRESS\",\"I-URL_PERSONAL\",\"O\"],\"y\":[39,78,1365,6,2,110,6,1,1096,15,20,1,4989794],\"type\":\"bar\",\"text\":[39.0,78.0,1365.0,6.0,2.0,110.0,6.0,1.0,1096.0,15.0,20.0,1.0,4989794.0],\"textposition\":\"outside\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis\":{\"title\":{\"text\":\"Count\"},\"type\":\"log\"},\"title\":{\"text\":\"Label Distribution\"},\"xaxis\":{\"title\":{\"text\":\"Labels\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d892c907-5551-473e-a753-0a7fef61a32b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_labels = np.array([x  for label in labels for x in label])\n",
    "unique_labels,label_counts = np.unique(all_labels,return_counts=True)\n",
    "\n",
    "fig = go.Figure(data=go.Bar(x=CFG.labels,y=label_counts))\n",
    "fig.update_layout(\n",
    "    title=\"Label Distribution\",\n",
    "    xaxis_title = 'Labels',\n",
    "    yaxis_title = 'Count',\n",
    "    yaxis_type = \"log\",\n",
    ")\n",
    "\n",
    "fig.update_traces(text=label_counts,textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:56.539576Z",
     "iopub.status.busy": "2024-05-17T05:36:56.539177Z",
     "iopub.status.idle": "2024-05-17T05:36:56.550294Z",
     "shell.execute_reply": "2024-05-17T05:36:56.549399Z",
     "shell.execute_reply.started": "2024-05-17T05:36:56.539541Z"
    }
   },
   "outputs": [],
   "source": [
    "train_words,valid_words,train_labels,valid_labels = train_test_split(\n",
    "    words,labels,test_size=0.2,random_state=CFG.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:56.552306Z",
     "iopub.status.busy": "2024-05-17T05:36:56.551483Z",
     "iopub.status.idle": "2024-05-17T05:36:56.632130Z",
     "shell.execute_reply": "2024-05-17T05:36:56.631302Z",
     "shell.execute_reply.started": "2024-05-17T05:36:56.552270Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['Visualization', 'tool', 'to', 'control', 'Humidity', '&', 'Temp',\n",
       "              'in', 'Spinning', 'Plant', '\\n\\n', 'Challenge', '&', 'Selection',\n",
       "              '\\n\\n', 'I', 'am', 'Electronics', 'Engineer', '&', 'working', 'as',\n",
       "              'Chief', 'Engineer', 'in', 'a', 'Spinning', 'plant', '.', 'We',\n",
       "              'have', 'to', 'face', '\\n\\n', 'challenges', '&', 'resolve', 'by',\n",
       "              'anonymous', 'way', '.', 'We', 'do', 'not', 'use', 'any', 'design',\n",
       "              'tool', 'to', 'resolve', 'any', 'problem', '.', 'Now', 'as', 'I',\n",
       "              'have', 'enrolled', 'in', 'Design', 'thinking', 'course', ',', 'I',\n",
       "              'started', 'to', 'apply', 'the', '1st', 'tool', 'as',\n",
       "              'Visualization', 'to', 'check', 'its', 'validity', 'whether', 'it',\n",
       "              'works', 'or', 'bookish', 'knowledge', 'only', '.', 'We', 'are',\n",
       "              'facing', 'a', 'chronic', 'problem', 'of', 'Humidity', '&',\n",
       "              'Temperature', 'Variation', 'in', 'Spinning', 'plant', '.',\n",
       "              'Humidity', '&', 'Temp', 'plays', 'a', 'vital', 'roll', 'to',\n",
       "              'control', 'the', 'yarn', 'strength', ',', 'breakage', '&', 'its',\n",
       "              'all', 'quality', 'parameter', '.', 'I', 'took', 'this',\n",
       "              'opportunity', 'to', 'resolve', 'this', 'problem', 'by',\n",
       "              'visualization', 'tool', '.', 'So', 'I', 'called', 'all',\n",
       "              'stakeholders', 'associated', 'with', 'this', 'problem', 'in',\n",
       "              'one', 'room', '&', 'explained', 'what', 'I', 'have', 'learn',\n",
       "              'from', 'the', 'visualization', 'tool', '.', 'Then', 'I', 'shared',\n",
       "              'the', 'visual', 'video', 'for', 'better', 'understanding', 'of',\n",
       "              'all', 'the', 'team', 'members', '.', '\\n\\n', 'Application',\n",
       "              '\\n\\n', 'I', 'took', 'the', 'challenge', 'as', 'Engineering',\n",
       "              'person', 'to', 'draw', 'a', 'picture', 'of', 'the', 'existing',\n",
       "              'operation', 'which', 'Existing', 'requirement', ',', 'Factors',\n",
       "              'that', 'can', 'effect', 'Humidity', '&', 'Temp', ',', 'Machine',\n",
       "              'condition', ',', 'Worker', 'efficiency', '&', 'its', 'capability',\n",
       "              '.', 'Existing', 'Set', 'up', 'parameters', 'were', 'pictured',\n",
       "              'as', 'temperature', 'as', 'thermometers', '&', 'digits', '.',\n",
       "              'Humidity', 'as', 'moisture', 'in', 'air', ',', 'Pressure',\n",
       "              'points', 'as', 'symbol', '.', 'Air', 'flow', 'as', 'vehicle', '.',\n",
       "              'Now', 'board', 'looked', 'very', 'interesting', '.', 'I',\n",
       "              'explained', 'how', 'our', 'existing', 'process', 'can', 'be',\n",
       "              'visualized', 'in', 'this', 'process', '.', 'Now', 'problem',\n",
       "              'has', 'been', 'explained', 'to', 'them', 'asked', 'to', 'refine',\n",
       "              'the', 'pictures', 'they', 'think', 'for', 'better', 'clarity',\n",
       "              'of', 'the', 'process', '.', 'Now', 'while', 'running', 'the',\n",
       "              'plant', 'we', 'started', 'to', 'visualize', '&', 'imagine',\n",
       "              'each', 'parameter', 'at', 'a', 'time', '.', 'A', 'dynamic',\n",
       "              'visualization', 'of', 'the', 'process', 'has', 'started',\n",
       "              'evolving', 'we', 'had', 'started', 'exploring', 'the',\n",
       "              'possibilities', 'process', 'parameters', 'which', 'can', 'lead',\n",
       "              'to', 'Humidity', '&', 'Temp', 'variation', 'in', 'the', 'plant',\n",
       "              '.', 'Operator', 'has', 'drawn', 'attention', 'some', 'parts',\n",
       "              'of', 'humidification', 'plant', 'which', 'vibrates', 'during',\n",
       "              'machine', 'running', '.', 'We', 'all', 'got', 'astonished', '&',\n",
       "              'operator', 'gave', 'us', 'the', 'problem', 'pin', 'points', '.',\n",
       "              'Now', 'engineering', 'team', 'has', 'fixed', 'that', 'problem',\n",
       "              '.', 'Another', 'department', 'person', 'told', 'that', 'door',\n",
       "              'of', 'the', 'plant', 'was', 'found', 'open', 'some', 'times',\n",
       "              'which', 'lead', 'to', 'enter', 'the', 'less', 'humidity', 'air',\n",
       "              'inside', 'the', 'plant', '.', 'This', 'may', 'be', 'the',\n",
       "              'reason', 'for', 'variation', '.', 'Another', 'observation', 'was',\n",
       "              'that', 'we', 'are', 'facing', 'problem', 'in', 'rainy', 'season',\n",
       "              'only', '&', 'very', 'less', 'variation', 'in', 'another',\n",
       "              'season', 'like', 'winter', '&', 'dry', 'season', '.', 'So', 'we',\n",
       "              'have', 'to', 'check', 'the', 'washer', 'pump', 'from', 'inside',\n",
       "              'for', 'their', 'efficiency', '.', '\\n\\n', 'Insight', '&',\n",
       "              'Approach', '\\n\\n', 'This', 'visualization', 'tool', 'process',\n",
       "              'helped', 'to', 'find', 'very', 'common', 'things', 'which', 'we',\n",
       "              'normally', 'ignore', 'while', 'thinking', 'for',\n",
       "              'troubleshooting', '.', 'When', 'we', 'write', 'or', 'draw', 'a',\n",
       "              'picture', ',', 'small', 'tiny', 'things', 'appear', 'in', 'the',\n",
       "              'picture', '.', 'These', 'small', 'details', 'pictures',\n",
       "              'actually', 'lead', 'to', 'complete', 'the', 'picture', 'as', 'a',\n",
       "              'whole', 'for', 'broader', 'vision', '.', 'So', 'design', 'tool',\n",
       "              'helped', 'us', 'for', 'a', 'systematic', 'approach', 'to',\n",
       "              'solve', 'the', 'problem', 'by', 'involving', 'all', 'related',\n",
       "              'stakeholders', '.', 'The', 'outcome', 'was', 'very',\n",
       "              'encouraging', '&', 'exciting', '\\n\\n'], dtype='<U15')             ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:56.633497Z",
     "iopub.status.busy": "2024-05-17T05:36:56.633159Z",
     "iopub.status.idle": "2024-05-17T05:36:58.231378Z",
     "shell.execute_reply": "2024-05-17T05:36:58.230319Z",
     "shell.execute_reply.started": "2024-05-17T05:36:56.633472Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'tokenizer.json' from model 'keras/deberta_v3/keras/deberta_v3_small_en/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/deberta_v3/keras/deberta_v3_small_en/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/deberta_v3/keras/deberta_v3_small_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DebertaV3Tokenizer name=deberta_v3_tokenizer, built=False>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras_nlp.models.DebertaV3Tokenizer.from_preset(CFG.preset)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:58.232802Z",
     "iopub.status.busy": "2024-05-17T05:36:58.232513Z",
     "iopub.status.idle": "2024-05-17T05:36:58.326606Z",
     "shell.execute_reply": "2024-05-17T05:36:58.325717Z",
     "shell.execute_reply.started": "2024-05-17T05:36:58.232777Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m        DebertaV3Tokenizer\n",
       "\u001b[0;31mString form:\u001b[0m <DebertaV3Tokenizer name=deberta_v3_tokenizer, built=False>\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/deberta_v3/deberta_v3_tokenizer.py\n",
       "\u001b[0;31mSource:\u001b[0m     \n",
       "\u001b[0;34m@\u001b[0m\u001b[0mkeras_nlp_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_nlp.models.DebertaV3Tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mDebertaV3Tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentencePieceTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"DeBERTa tokenizer layer based on SentencePiece.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This tokenizer class will tokenize raw strings into integer sequences and\u001b[0m\n",
       "\u001b[0;34m    is based on `keras_nlp.tokenizers.SentencePieceTokenizer`. Unlike the\u001b[0m\n",
       "\u001b[0;34m    underlying tokenizer, it will check for all special tokens needed by\u001b[0m\n",
       "\u001b[0;34m    DeBERTa models and provides a `from_preset()` method to automatically\u001b[0m\n",
       "\u001b[0;34m    download a matching vocabulary for a DeBERTa preset.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This tokenizer does not provide truncation or padding of inputs. It can be\u001b[0m\n",
       "\u001b[0;34m    combined with a `keras_nlp.models.DebertaV3Preprocessor` layer for input\u001b[0m\n",
       "\u001b[0;34m    packing.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    If input is a batch of strings (rank > 0), the layer will output a\u001b[0m\n",
       "\u001b[0;34m    `tf.RaggedTensor` where the last dimension of the output is ragged.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    If input is a scalar string (rank == 0), the layer will output a dense\u001b[0m\n",
       "\u001b[0;34m    `tf.Tensor` with static shape `[None]`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Note: The mask token (`\"[MASK]\"`) is handled differently in this tokenizer.\u001b[0m\n",
       "\u001b[0;34m    If the token is not present in the provided SentencePiece vocabulary, the\u001b[0m\n",
       "\u001b[0;34m    token will be appended to the vocabulary. For example, if the vocabulary\u001b[0m\n",
       "\u001b[0;34m    size is 100, the mask token will be assigned the ID 100.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        proto: Either a `string` path to a SentencePiece proto file, or a\u001b[0m\n",
       "\u001b[0;34m            `bytes` object with a serialized SentencePiece proto. See the\u001b[0m\n",
       "\u001b[0;34m            [SentencePiece repository](https://github.com/google/sentencepiece)\u001b[0m\n",
       "\u001b[0;34m            for more details on the format.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    ```python\u001b[0m\n",
       "\u001b[0;34m    # Unbatched input.\u001b[0m\n",
       "\u001b[0;34m    tokenizer = keras_nlp.models.DebertaV3Tokenizer.from_preset(\u001b[0m\n",
       "\u001b[0;34m        \"deberta_v3_base_en\",\u001b[0m\n",
       "\u001b[0;34m    )\u001b[0m\n",
       "\u001b[0;34m    tokenizer(\"The quick brown fox jumped.\")\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    # Batched inputs.\u001b[0m\n",
       "\u001b[0;34m    tokenizer([\"the quick brown fox\", \"the earth is round\"])\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    # Detokenization.\u001b[0m\n",
       "\u001b[0;34m    tokenizer.detokenize(tokenizer(\"The quick brown fox jumped.\"))\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    # Custom vocabulary.\u001b[0m\n",
       "\u001b[0;34m    bytes_io = io.BytesIO()\u001b[0m\n",
       "\u001b[0;34m    ds = tf.data.Dataset.from_tensor_slices([\"The quick brown fox jumped.\"])\u001b[0m\n",
       "\u001b[0;34m    sentencepiece.SentencePieceTrainer.train(\u001b[0m\n",
       "\u001b[0;34m        sentence_iterator=ds.as_numpy_iterator(),\u001b[0m\n",
       "\u001b[0;34m        model_writer=bytes_io,\u001b[0m\n",
       "\u001b[0;34m        vocab_size=9,\u001b[0m\n",
       "\u001b[0;34m        model_type=\"WORD\",\u001b[0m\n",
       "\u001b[0;34m        pad_id=0,\u001b[0m\n",
       "\u001b[0;34m        bos_id=1,\u001b[0m\n",
       "\u001b[0;34m        eos_id=2,\u001b[0m\n",
       "\u001b[0;34m        unk_id=3,\u001b[0m\n",
       "\u001b[0;34m        pad_piece=\"[PAD]\",\u001b[0m\n",
       "\u001b[0;34m        bos_piece=\"[CLS]\",\u001b[0m\n",
       "\u001b[0;34m        eos_piece=\"[SEP]\",\u001b[0m\n",
       "\u001b[0;34m        unk_piece=\"[UNK]\",\u001b[0m\n",
       "\u001b[0;34m    )\u001b[0m\n",
       "\u001b[0;34m    tokenizer = keras_nlp.models.DebertaV3Tokenizer(\u001b[0m\n",
       "\u001b[0;34m        proto=bytes_io.getvalue(),\u001b[0m\n",
       "\u001b[0;34m    )\u001b[0m\n",
       "\u001b[0;34m    tokenizer(\"The quick brown fox jumped.\")\u001b[0m\n",
       "\u001b[0;34m    ```\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[PAD]\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mset_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0;34mf\"Cannot find token `'{token}'` in the provided \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0;34mf\"`vocabulary`. Please provide `'{token}'` in your \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0;34m\"`vocabulary` or use a pretrained `vocabulary` name.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# If the mask token is not in the vocabulary, add it to the end of the\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msentence_piece_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msentence_piece_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0msentence_piece_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msentence_piece_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msentence_piece_vocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0msentence_piece_vocabulary\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msentence_piece_vocabulary\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mid_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboolean_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:58.328411Z",
     "iopub.status.busy": "2024-05-17T05:36:58.327780Z",
     "iopub.status.idle": "2024-05-17T05:36:58.333510Z",
     "shell.execute_reply": "2024-05-17T05:36:58.332697Z",
     "shell.execute_reply.started": "2024-05-17T05:36:58.328376Z"
    }
   },
   "outputs": [],
   "source": [
    "packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "    start_value=tokenizer.cls_token_id,\n",
    "    end_value=tokenizer.sep_token_id,\n",
    "    sequence_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:58.334968Z",
     "iopub.status.busy": "2024-05-17T05:36:58.334689Z",
     "iopub.status.idle": "2024-05-17T05:36:58.345558Z",
     "shell.execute_reply": "2024-05-17T05:36:58.344602Z",
     "shell.execute_reply.started": "2024-05-17T05:36:58.334933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MultiSegmentPacker name=multi_segment_packer, built=False>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:58.347129Z",
     "iopub.status.busy": "2024-05-17T05:36:58.346818Z",
     "iopub.status.idle": "2024-05-17T05:36:58.357958Z",
     "shell.execute_reply": "2024-05-17T05:36:58.357120Z",
     "shell.execute_reply.started": "2024-05-17T05:36:58.347106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Design', 'Thinking', 'for', 'innovation', 'reflexion'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_words = words[0][:5]\n",
    "sample_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:36:58.359187Z",
     "iopub.status.busy": "2024-05-17T05:36:58.358928Z",
     "iopub.status.idle": "2024-05-17T05:37:00.010675Z",
     "shell.execute_reply": "2024-05-17T05:37:00.009682Z",
     "shell.execute_reply.started": "2024-05-17T05:36:58.359165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words        : ['Design', 'Thinking', 'for', 'innovation', 'reflexion']\n",
      "tokens (str) : ['▁Design', '▁Thinking', '▁for', '▁innovation', '▁reflex', 'ion']\n",
      "tokens (int) : [2169, 12103, 270, 3513, 28310, 4593]\n"
     ]
    }
   ],
   "source": [
    "sample_token_int = [\n",
    "    token.tolist() for word in sample_words for token in tokenizer(word)\n",
    "]\n",
    "sample_token_str = [tokenizer.id_to_token(token) for token in sample_token_int]\n",
    "\n",
    "print(\"words        :\", sample_words.tolist())\n",
    "print(\"tokens (str) :\", sample_token_str)\n",
    "print(\"tokens (int) :\", sample_token_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.012545Z",
     "iopub.status.busy": "2024-05-17T05:37:00.012148Z",
     "iopub.status.idle": "2024-05-17T05:37:00.110985Z",
     "shell.execute_reply": "2024-05-17T05:37:00.110090Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.012501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens (str)        : ['▁Design', '▁Thinking', '▁for', '▁innovation', '▁reflex', 'ion']\n",
      "padded tokens (str) : ['[CLS]', '▁Design', '▁Thinking', '▁for', '▁innovation', '▁reflex', 'ion', '[SEP]', '[PAD]', '[PAD]'] \n",
      "\n",
      "tokens (int)        : [2169, 12103, 270, 3513, 28310, 4593]\n",
      "padded tokens (int) : [1, 2169, 12103, 270, 3513, 28310, 4593, 2, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/backend/jax/core.py:64: UserWarning:\n",
      "\n",
      "Explicitly requested dtype int64 requested in asarray is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "padded_sample_tokens_int = packer(np.array(sample_token_int))[0].tolist()\n",
    "padded_sample_tokens_str = [\n",
    "    tokenizer.id_to_token(token) for token in padded_sample_tokens_int\n",
    "]\n",
    "\n",
    "print(\"tokens (str)        :\", sample_token_str)\n",
    "print(\"padded tokens (str) :\", padded_sample_tokens_str, \"\\n\")\n",
    "\n",
    "print(\"tokens (int)        :\", sample_token_int)\n",
    "print(\"padded tokens (int) :\", padded_sample_tokens_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.112329Z",
     "iopub.status.busy": "2024-05-17T05:37:00.112067Z",
     "iopub.status.idle": "2024-05-17T05:37:00.127957Z",
     "shell.execute_reply": "2024-05-17T05:37:00.126864Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.112306Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "token_ids = tf.constant([101, 202, 202, 303, 404, 404, 505, 102, 606, 606, 707])\n",
    "mask = tf.concat([[True], token_ids[1:] != token_ids[:-1]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.129502Z",
     "iopub.status.busy": "2024-05-17T05:37:00.129071Z",
     "iopub.status.idle": "2024-05-17T05:37:00.137408Z",
     "shell.execute_reply": "2024-05-17T05:37:00.136322Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.129470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([202, 202, 303, 404, 404, 505, 102, 606, 606, 707], dtype=int32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.138814Z",
     "iopub.status.busy": "2024-05-17T05:37:00.138488Z",
     "iopub.status.idle": "2024-05-17T05:37:00.148812Z",
     "shell.execute_reply": "2024-05-17T05:37:00.147823Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.138783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([101, 202, 202, 303, 404, 404, 505, 102, 606, 606], dtype=int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.150307Z",
     "iopub.status.busy": "2024-05-17T05:37:00.150013Z",
     "iopub.status.idle": "2024-05-17T05:37:00.163772Z",
     "shell.execute_reply": "2024-05-17T05:37:00.162815Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.150284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=bool, numpy=\n",
       "array([ True, False,  True,  True, False,  True,  True,  True, False,\n",
       "        True])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[1:] != token_ids[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.165456Z",
     "iopub.status.busy": "2024-05-17T05:37:00.164968Z",
     "iopub.status.idle": "2024-05-17T05:37:00.185840Z",
     "shell.execute_reply": "2024-05-17T05:37:00.184770Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.165428Z"
    }
   },
   "outputs": [],
   "source": [
    "#字符处理\n",
    "def get_tokens(words, seq_len, packer):\n",
    "    # Tokenize input  将单词列表转换成id\n",
    "    token_words = tf.expand_dims( \n",
    "        tokenizer(words), axis=-1\n",
    "    )  # ex: (words) [\"It's\", \"a\", \"cat\"] ->  (token_words) [[1, 2], [3], [4]]\n",
    "    # 将id列表重塑为一列\n",
    "    tokens = tf.reshape(\n",
    "        token_words, [-1]\n",
    "    )  # ex: (token_words) [[1, 2], [3], [4]] -> (tokens) [1, 2, 3, 4]\n",
    "    # Pad tokens\n",
    "    tokens = packer(tokens)[0][:seq_len] #进行特殊字符标记\n",
    "    inputs = {\"token_ids\": tokens, \"padding_mask\": tokens != 0} #确定input数组\n",
    "    return inputs, tokens, token_words \n",
    "\n",
    "\n",
    "def get_token_ids(token_words):\n",
    "    # Get word indices   单词索引\n",
    "    word_ids = tf.range(tf.shape(token_words)[0])\n",
    "    # Get size of each word  实验map_fn统计长度维度的length 维度自动确定为一列\n",
    "    word_size = tf.reshape(tf.map_fn(lambda word: tf.shape(word)[0:1], token_words), [-1])\n",
    "    # Repeat word_id with size of word to get token_id  # 将单词的id重复对应wordsize次数\n",
    "    token_ids = tf.repeat(word_ids, word_size)   \n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def get_token_labels(word_labels, token_ids, seq_len):\n",
    "    # Create token_labels from word_labels ->  alignment  将word和token_ids进行对齐\n",
    "    token_labels = tf.gather(word_labels, token_ids) \n",
    "    # Only label the first token of a given word and assign -100 to others  \n",
    "    mask = tf.concat([[True], token_ids[1:] != token_ids[:-1]], axis=0) # 标记每个单词开始的位置\n",
    "    token_labels = tf.where(mask, token_labels, -100) #每个单词除去开头的id，后续被mask\n",
    "    # Truncate to max sequence length \n",
    "    token_labels = token_labels[: seq_len - 2]  # -2 for special tokens ([CLS], [SEP]) 取出最后两个特殊字符\n",
    "    # Pad token_labels to align with tokens (use -100 to pad for loss/metric ignore)\n",
    "    pad_start = 1  # for [CLS] token 为cls留出一个位置\n",
    "    pad_end = seq_len - tf.shape(token_labels)[0] - 1  # for [SEP] and [PAD] tokens 为了sep留出最后一个位置\n",
    "    token_labels = tf.pad(token_labels, [[pad_start, pad_end]], constant_values=-100) #标记pad\n",
    "    return token_labels\n",
    "\n",
    "\n",
    "def process_token_ids(token_ids, seq_len):\n",
    "    # Truncate to max sequence length\n",
    "    token_ids = token_ids[: seq_len - 2]  # -2 for special tokens ([CLS], [SEP])\n",
    "    # Pad token_ids to align with tokens (use -1 to pad for later identification)\n",
    "    pad_start = 1  # [CLS] token\n",
    "    pad_end = seq_len - tf.shape(token_ids)[0] - 1  # [SEP] and [PAD] tokens\n",
    "    token_ids = tf.pad(token_ids, [[pad_start, pad_end]], constant_values=-1)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def process_data(seq_len=720, has_label=True, return_ids=False):\n",
    "    # To add spetical tokens: [CLS], [SEP], [PAD]   标记特殊符号\n",
    "    packer = keras_nlp.layers.MultiSegmentPacker(\n",
    "        start_value=tokenizer.cls_token_id,\n",
    "        end_value=tokenizer.sep_token_id,\n",
    "        sequence_length=seq_len,\n",
    "    )\n",
    "\n",
    "    def process(x):\n",
    "        # Generate inputs from tokens 转换为tokenize列表\n",
    "        inputs, tokens, words_int = get_tokens(x[\"words\"], seq_len, packer) \n",
    "        # Generate token_ids for maping tokens to words\n",
    "        token_ids = get_token_ids(words_int)  # 单词token重复对应字数次\n",
    "        if has_label:\n",
    "            # Generate token_labels from word_labels  获取单词分割后的序列以及pad标记\n",
    "            token_labels = get_token_labels(x[\"labels\"], token_ids, seq_len)\n",
    "            return inputs, token_labels  #返回标记words和 tokens\n",
    "        elif return_ids:\n",
    "            # Pad token_ids to align with tokens\n",
    "            token_ids = process_token_ids(token_ids, seq_len)  # 直接返回没有label的pad序列\n",
    "            return token_ids\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "#构造数据集\n",
    "def build_dataset(words, labels=None, return_ids=False, batch_size=4,\n",
    "                  seq_len=512, shuffle=False, cache=True, drop_remainder=True):\n",
    "    AUTO = tf.data.AUTOTUNE  #自动选择数据并行数量\n",
    "\n",
    "    slices = {\"words\": tf.ragged.constant(words)} #将不同长度word文本转换成一个Ragged Tensor\n",
    "    if labels is not None:\n",
    "        slices.update({\"labels\": tf.ragged.constant(labels)}) #将不同长度label文本转换成一个Ragged Tensor\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices) # 从slices变为数据集\n",
    "    ds = ds.map(process_data(seq_len=seq_len,\n",
    "                             has_label=labels is not None, \n",
    "                             return_ids=return_ids), num_parallel_calls=AUTO) # apply processing\n",
    "    ds = ds.cache() if cache else ds  # cache dataset 数据加载到内存\n",
    "    if shuffle: # shuffle dataset\n",
    "        ds = ds.shuffle(1024, seed=CFG.seed)  #使用一个固定大小的缓冲区（1024个元素）来随机打乱数据\n",
    "        opt = tf.data.Options()  # 确保多线程中的实验不确定性问题 \n",
    "        opt.experimental_deterministic = False  # 此处表示当数据集不能被bs整除时，保留最后一个不完整批次\n",
    "        ds = ds.with_options(opt)  #加载配置\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # batch dataset 数据集划分batch\n",
    "    ds = ds.prefetch(AUTO)  # prefetch next batch  自动获取下一个批次\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:00.187229Z",
     "iopub.status.busy": "2024-05-17T05:37:00.186879Z",
     "iopub.status.idle": "2024-05-17T05:37:34.036627Z",
     "shell.execute_reply": "2024-05-17T05:37:34.035660Z",
     "shell.execute_reply.started": "2024-05-17T05:37:00.187206Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = build_dataset(train_words, train_labels,  batch_size=CFG.train_batch_size,\n",
    "                         seq_len=CFG.train_seq_len, shuffle=True)\n",
    "\n",
    "valid_ds = build_dataset(valid_words, valid_labels, batch_size=CFG.train_batch_size, \n",
    "                         seq_len=CFG.train_seq_len, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:34.038327Z",
     "iopub.status.busy": "2024-05-17T05:37:34.037936Z",
     "iopub.status.idle": "2024-05-17T05:37:34.679526Z",
     "shell.execute_reply": "2024-05-17T05:37:34.678448Z",
     "shell.execute_reply.started": "2024-05-17T05:37:34.038293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Input:\n",
      " {'token_ids': <tf.Tensor: shape=(16, 1024), dtype=int32, numpy=\n",
      "array([[    1, 28525,   877, ...,     0,     0,     0],\n",
      "       [    1, 45730,   377, ...,     0,     0,     0],\n",
      "       [    1,  8489,  7933, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    1,  6348, 28525, ...,  1068,  3955,     2],\n",
      "       [    1,  3780, 13942, ...,     0,     0,     0],\n",
      "       [    1, 45730,   377, ...,  6568,   366,     2]], dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(16, 1024), dtype=bool, numpy=\n",
      "array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False],\n",
      "       ...,\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ...,  True,  True,  True]])>}\n",
      "\n",
      "# Labels:\n",
      " tf.Tensor(\n",
      "[[-100   12   12 ... -100 -100 -100]\n",
      " [-100   12   12 ... -100 -100 -100]\n",
      " [-100   12 -100 ... -100 -100 -100]\n",
      " ...\n",
      " [-100   12   12 ...   12   12 -100]\n",
      " [-100   12 -100 ... -100 -100 -100]\n",
      " [-100   12   12 ...   12   12 -100]], shape=(16, 1024), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inp, tar = next(iter(valid_ds))\n",
    "print(\"# Input:\\n\",inp); print(\"\\n# Labels:\\n\",tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:34.681231Z",
     "iopub.status.busy": "2024-05-17T05:37:34.680892Z",
     "iopub.status.idle": "2024-05-17T05:37:34.689609Z",
     "shell.execute_reply": "2024-05-17T05:37:34.688595Z",
     "shell.execute_reply.started": "2024-05-17T05:37:34.681203Z"
    }
   },
   "outputs": [],
   "source": [
    "#交叉熵定义\n",
    "class CrossEntropy(keras.losses.SparseCategoricalCrossentropy):  # 交叉熵实现\n",
    "    def __init__(self, ignore_class=-100, reduction=None, **args):\n",
    "        super().__init__(reduction=reduction, **args)\n",
    "        self.ignore_class = ignore_class\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = ops.reshape(y_true, [-1])\n",
    "        y_pred = ops.reshape(y_pred, [-1, CFG.num_labels])\n",
    "        loss = super().call(y_true, y_pred)\n",
    "        if self.ignore_class is not None:\n",
    "            valid_mask = ops.not_equal(\n",
    "                y_true, ops.cast(self.ignore_class, y_pred.dtype)\n",
    "            )\n",
    "            loss = ops.where(valid_mask, loss, 0.0)\n",
    "            loss = ops.sum(loss)\n",
    "            loss /= ops.maximum(ops.sum(ops.cast(valid_mask, loss.dtype)), 1)\n",
    "        else:\n",
    "            loss = ops.mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:34.691902Z",
     "iopub.status.busy": "2024-05-17T05:37:34.691006Z",
     "iopub.status.idle": "2024-05-17T05:37:34.706990Z",
     "shell.execute_reply": "2024-05-17T05:37:34.706164Z",
     "shell.execute_reply.started": "2024-05-17T05:37:34.691868Z"
    }
   },
   "outputs": [],
   "source": [
    "# F beta 评价指标\n",
    "class FBetaScore(keras.metrics.FBetaScore):\n",
    "    def __init__(self, ignore_classes=[-100, 12], average=\"micro\", beta=5.0,\n",
    "                 name=\"f5_score\", **args):\n",
    "        super().__init__(beta=beta, average=average, name=name, **args)\n",
    "        self.ignore_classes = ignore_classes or []\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = ops.convert_to_tensor(y_true, dtype=self.dtype)\n",
    "        y_pred = ops.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "        \n",
    "        y_true = ops.reshape(y_true, [-1])\n",
    "        y_pred = ops.reshape(y_pred, [-1, CFG.num_labels])\n",
    "            \n",
    "        valid_mask = ops.ones_like(y_true, dtype=self.dtype)\n",
    "        if self.ignore_classes:\n",
    "            for ignore_class in self.ignore_classes:\n",
    "                valid_mask &= ops.not_equal(y_true, ops.cast(ignore_class, y_pred.dtype))\n",
    "        valid_mask = ops.expand_dims(valid_mask, axis=-1)\n",
    "        \n",
    "        y_true = ops.one_hot(y_true, CFG.num_labels)\n",
    "        \n",
    "        if not self._built:\n",
    "            self._build(y_true.shape, y_pred.shape)\n",
    "\n",
    "        threshold = ops.max(y_pred, axis=-1, keepdims=True)\n",
    "        y_pred = ops.logical_and(\n",
    "            y_pred >= threshold, ops.abs(y_pred) > 1e-9\n",
    "        )\n",
    "\n",
    "        y_pred = ops.cast(y_pred, dtype=self.dtype)\n",
    "        y_true = ops.cast(y_true, dtype=self.dtype)\n",
    "        \n",
    "        tp = ops.sum(y_pred * y_true * valid_mask, self.axis)\n",
    "        fp = ops.sum(y_pred * (1 - y_true) * valid_mask, self.axis)\n",
    "        fn = ops.sum((1 - y_pred) * y_true * valid_mask, self.axis)\n",
    "            \n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:34.708659Z",
     "iopub.status.busy": "2024-05-17T05:37:34.708230Z",
     "iopub.status.idle": "2024-05-17T05:37:49.309200Z",
     "shell.execute_reply": "2024-05-17T05:37:49.308267Z",
     "shell.execute_reply.started": "2024-05-17T05:37:34.708632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/deberta_v3/keras/deberta_v3_small_en/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/deberta_v3/keras/deberta_v3_small_en/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/deberta_v3/keras/deberta_v3_small_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\"> Param # </span>┃<span style=\"font-weight: bold\"> Connected to         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">98,380…</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbeddi…</span> │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ embeddings_layer_n… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ embeddings_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embeddings_layer_no… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ rel_embedding       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ embeddings_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RelativeEmbedding</span>) │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,…</span> │ embeddings_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │         │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │         │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,…</span> │ disentangled_attent… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │         │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │         │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,…</span> │ disentangled_attent… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │         │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │         │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,…</span> │ disentangled_attent… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │         │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │         │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,…</span> │ disentangled_attent… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │         │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │         │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,…</span> │ disentangled_attent… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DisentangledAtten…</span> │                   │         │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │         │ rel_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)  │   <span style=\"color: #00af00; text-decoration-color: #00af00\">9,997</span> │ disentangled_attent… │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ prediction          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ logits[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │         │                      │\n",
       "└─────────────────────┴───────────────────┴─────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mParam #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to        \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │       \u001b[38;5;34m0\u001b[0m │ -                    │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m98,380…\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mReversibleEmbeddi…\u001b[0m │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ embeddings_layer_n… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │   \u001b[38;5;34m1,536\u001b[0m │ token_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ embeddings_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │       \u001b[38;5;34m0\u001b[0m │ embeddings_layer_no… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │       \u001b[38;5;34m0\u001b[0m │ -                    │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ rel_embedding       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │ \u001b[38;5;34m394,752\u001b[0m │ embeddings_dropout[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mRelativeEmbedding\u001b[0m) │                   │         │                      │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m7,087,…\u001b[0m │ embeddings_dropout[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │         │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │         │ rel_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m7,087,…\u001b[0m │ disentangled_attent… │\n",
       "│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │         │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │         │ rel_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m7,087,…\u001b[0m │ disentangled_attent… │\n",
       "│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │         │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │         │ rel_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m7,087,…\u001b[0m │ disentangled_attent… │\n",
       "│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │         │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │         │ rel_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m7,087,…\u001b[0m │ disentangled_attent… │\n",
       "│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │         │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │         │ rel_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ disentangled_atten… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m) │ \u001b[38;5;34m7,087,…\u001b[0m │ disentangled_attent… │\n",
       "│ (\u001b[38;5;33mDisentangledAtten…\u001b[0m │                   │         │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │         │ rel_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)  │   \u001b[38;5;34m9,997\u001b[0m │ disentangled_attent… │\n",
       "├─────────────────────┼───────────────────┼─────────┼──────────────────────┤\n",
       "│ prediction          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)  │       \u001b[38;5;34m0\u001b[0m │ logits[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │         │                      │\n",
       "└─────────────────────┴───────────────────┴─────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,314,317</span> (539.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m141,314,317\u001b[0m (539.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,314,317</span> (539.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m141,314,317\u001b[0m (539.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Token Classification model  建立基于tokens分类的模型管道\n",
    "backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n",
    "    CFG.preset,\n",
    ")\n",
    "out = backbone.output\n",
    "out = keras.layers.Dense(CFG.num_labels, name=\"logits\")(out) #修改输出头\n",
    "out = keras.layers.Activation(\"softmax\", dtype=\"float32\", name=\"prediction\")(out) #加一层分类激活函数\n",
    "model = keras.models.Model(backbone.input, out) #建立管道\n",
    "\n",
    "# Compile model for optimizer, loss and metric\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=2e-5), #\n",
    "    loss=CrossEntropy(),\n",
    "    metrics=[FBetaScore()],\n",
    ")\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:49.310894Z",
     "iopub.status.busy": "2024-05-17T05:37:49.310589Z",
     "iopub.status.idle": "2024-05-17T05:37:49.321221Z",
     "shell.execute_reply": "2024-05-17T05:37:49.320300Z",
     "shell.execute_reply.started": "2024-05-17T05:37:49.310868Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#详细定义学习率schedule\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 6e-6, 2.5e-6 * batch_size, 1e-6\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        fig = px.line(x=np.arange(epochs),\n",
    "                      y=[lrfn(epoch) for epoch in np.arange(epochs)], \n",
    "                      title='LR Scheduler',\n",
    "                      markers=True,\n",
    "                      labels={'x': 'epoch', 'y': 'lr'})\n",
    "        fig.update_layout(\n",
    "            yaxis = dict(\n",
    "                showexponent = 'all',\n",
    "                exponentformat = 'e'\n",
    "            )\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:49.322845Z",
     "iopub.status.busy": "2024-05-17T05:37:49.322515Z",
     "iopub.status.idle": "2024-05-17T05:37:51.274850Z",
     "shell.execute_reply": "2024-05-17T05:37:51.273806Z",
     "shell.execute_reply.started": "2024-05-17T05:37:49.322813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"837c4009-6bea-465c-8d73-3b8f814787ea\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"837c4009-6bea-465c-8d73-3b8f814787ea\")) {                    Plotly.newPlot(                        \"837c4009-6bea-465c-8d73-3b8f814787ea\",                        [{\"hovertemplate\":\"epoch=%{x}\\u003cbr\\u003elr=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9],\"xaxis\":\"x\",\"y\":[6e-6,0.000017333333333333332,0.000028666666666666668,0.00004,0.000030250000000000007,0.000022937500000000005,0.000017453125000000002,0.000013339843750000002,0.000010254882812500002,7.941162109375002e-6],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"lr\"},\"showexponent\":\"all\",\"exponentformat\":\"e\"},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"LR Scheduler\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('837c4009-6bea-465c-8d73-3b8f814787ea');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_cb = get_lr_callback(CFG.train_batch_size, mode=CFG.lr_mode, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T05:37:51.276454Z",
     "iopub.status.busy": "2024-05-17T05:37:51.276133Z",
     "iopub.status.idle": "2024-05-17T06:31:44.340543Z",
     "shell.execute_reply": "2024-05-17T06:31:44.339552Z",
     "shell.execute_reply.started": "2024-05-17T05:37:51.276427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 2s/step - f5_score: 0.0254 - loss: 0.4733 - val_f5_score: 0.0111 - val_loss: 0.0044 - learning_rate: 6.0000e-06\n",
      "Epoch 2/6\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 2s/step - f5_score: 0.0930 - loss: 0.0031 - val_f5_score: 0.5498 - val_loss: 0.0013 - learning_rate: 1.7333e-05\n",
      "Epoch 3/6\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 2s/step - f5_score: 0.5493 - loss: 0.0014 - val_f5_score: 0.8506 - val_loss: 7.0660e-04 - learning_rate: 2.8667e-05\n",
      "Epoch 4/6\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 2s/step - f5_score: 0.7961 - loss: 6.2621e-04 - val_f5_score: 0.9114 - val_loss: 5.2849e-04 - learning_rate: 4.0000e-05\n",
      "Epoch 5/6\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 2s/step - f5_score: 0.9310 - loss: 2.5756e-04 - val_f5_score: 0.8469 - val_loss: 4.4582e-04 - learning_rate: 3.0250e-05\n",
      "Epoch 6/6\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 2s/step - f5_score: 0.9630 - loss: 1.3233e-04 - val_f5_score: 0.8967 - val_loss: 4.0003e-04 - learning_rate: 2.2938e-05\n"
     ]
    }
   ],
   "source": [
    "#建立数据以及参数，模型训练的pipline\n",
    "if CFG.train:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        callbacks=[lr_cb],\n",
    "        verbose=1,\n",
    "    )\n",
    "else:\n",
    "    model.load_weights(\"/kaggle/input/pii-data-detection-ckpt-ds/model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:31:44.342957Z",
     "iopub.status.busy": "2024-05-17T06:31:44.342077Z",
     "iopub.status.idle": "2024-05-17T06:31:50.973113Z",
     "shell.execute_reply": "2024-05-17T06:31:50.972144Z",
     "shell.execute_reply.started": "2024-05-17T06:31:44.342898Z"
    }
   },
   "outputs": [],
   "source": [
    "#构建验证集合\n",
    "# Build Validation dataloader with \"infer_seq_len\"\n",
    "valid_ds = build_dataset(valid_words, valid_labels, return_ids=False, batch_size=CFG.infer_batch_size,\n",
    "                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:39:21.877284Z",
     "iopub.status.busy": "2024-05-17T06:39:21.876612Z",
     "iopub.status.idle": "2024-05-17T06:41:56.738419Z",
     "shell.execute_reply": "2024-05-17T06:41:56.737437Z",
     "shell.execute_reply.started": "2024-05-17T06:39:21.877252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f5_score': 0.8926653861999512, 'loss': 0.0004159419040661305}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_ds,return_dict=True,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:50:33.683343Z",
     "iopub.status.busy": "2024-05-17T06:50:33.682961Z",
     "iopub.status.idle": "2024-05-17T06:50:35.418710Z",
     "shell.execute_reply": "2024-05-17T06:50:35.417901Z",
     "shell.execute_reply.started": "2024-05-17T06:50:33.683310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87caf2891748442390c1fc7a2d5642af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = json.load(open(f\"{BASE_PATH}/test.json\"))\n",
    "#增补末尾样本，对齐batch数量\n",
    "need_samples = len(devices) - len(test_data) % len(devices)\n",
    "for _ in range(need_samples):\n",
    "    test_data.append(test_data[-1])\n",
    "\n",
    "#创建容器\n",
    "test_words = np.empty(len(test_data),dtype=object)\n",
    "test_docs = np.empty(len(test_data),dtype=np.int32)\n",
    "\n",
    "#将字段赋值到数组中\n",
    "for i,x in tqdm(enumerate(test_data),total=len(test_data)):\n",
    "    test_words[i] = np.array(x['tokens'])\n",
    "    test_docs[i] = x['document']\n",
    "\n",
    "#测试集构建    \n",
    "id_ds = build_dataset(test_words,return_ids=True, batch_size=len(test_words), \n",
    "                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False, drop_remainder=False)\n",
    "#只取出测试集的token ids\n",
    "test_token_ids = ops.convert_to_numpy([ids for ids in iter(id_ds)][0])\n",
    "\n",
    "# Build test dataloader \n",
    "test_ds = build_dataset(test_words, return_ids=False, batch_size=CFG.infer_batch_size,\n",
    "                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:51:44.339178Z",
     "iopub.status.busy": "2024-05-17T06:51:44.338757Z",
     "iopub.status.idle": "2024-05-17T06:51:49.618510Z",
     "shell.execute_reply": "2024-05-17T06:51:49.617432Z",
     "shell.execute_reply.started": "2024-05-17T06:51:44.339145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.3834994e-01, 5.3652093e-02, 2.8588483e-02, ...,\n",
       "         4.8667604e-01, 6.1542470e-02, 1.7658062e-02],\n",
       "        [3.1376530e-07, 2.9160586e-07, 4.2788447e-06, ...,\n",
       "         1.1078923e-07, 9.7580390e-08, 9.9998879e-01],\n",
       "        [3.9509092e-07, 1.2876927e-07, 2.3466221e-06, ...,\n",
       "         2.9619855e-07, 9.5506479e-08, 9.9999070e-01],\n",
       "        ...,\n",
       "        [2.6591924e-05, 9.2182614e-05, 1.2974467e-04, ...,\n",
       "         5.1558491e-05, 2.1077069e-05, 9.9635363e-01],\n",
       "        [2.6591924e-05, 9.2182614e-05, 1.2974467e-04, ...,\n",
       "         5.1558491e-05, 2.1077069e-05, 9.9635363e-01],\n",
       "        [2.6591924e-05, 9.2182614e-05, 1.2974467e-04, ...,\n",
       "         5.1558491e-05, 2.1077069e-05, 9.9635363e-01]],\n",
       "\n",
       "       [[1.3684441e-01, 5.3406183e-02, 2.8453974e-02, ...,\n",
       "         4.8091015e-01, 6.4893723e-02, 1.9692658e-02],\n",
       "        [6.9254870e-04, 2.2013328e-05, 9.6806484e-01, ...,\n",
       "         9.5776003e-04, 3.9789148e-04, 1.7118210e-02],\n",
       "        [2.9730154e-04, 7.9593847e-05, 4.0494069e-05, ...,\n",
       "         2.8911560e-05, 1.6973882e-04, 2.6486877e-03],\n",
       "        ...,\n",
       "        [1.6077103e-04, 3.7636788e-04, 1.9254042e-03, ...,\n",
       "         3.8043267e-04, 2.5691136e-04, 9.7380418e-01],\n",
       "        [1.6077103e-04, 3.7636788e-04, 1.9254042e-03, ...,\n",
       "         3.8043267e-04, 2.5691136e-04, 9.7380418e-01],\n",
       "        [1.6077103e-04, 3.7636788e-04, 1.9254042e-03, ...,\n",
       "         3.8043267e-04, 2.5691136e-04, 9.7380418e-01]],\n",
       "\n",
       "       [[1.3590500e-01, 5.4991081e-02, 2.9251926e-02, ...,\n",
       "         4.7528246e-01, 6.4985432e-02, 2.2513263e-02],\n",
       "        [7.2303997e-08, 3.1127950e-08, 1.8793701e-06, ...,\n",
       "         7.7419507e-08, 2.8453247e-08, 9.9999642e-01],\n",
       "        [1.6021808e-07, 1.9762568e-07, 3.1669285e-07, ...,\n",
       "         7.8946641e-08, 1.9593886e-08, 9.9999857e-01],\n",
       "        ...,\n",
       "        [5.1613988e-05, 1.6808281e-04, 5.7754043e-04, ...,\n",
       "         1.6148591e-04, 8.8141991e-05, 9.8966414e-01],\n",
       "        [5.1613988e-05, 1.6808281e-04, 5.7754043e-04, ...,\n",
       "         1.6148591e-04, 8.8141991e-05, 9.8966414e-01],\n",
       "        [5.1613988e-05, 1.6808281e-04, 5.7754043e-04, ...,\n",
       "         1.6148591e-04, 8.8141991e-05, 9.8966414e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.2631206e-01, 5.5343550e-02, 3.0285068e-02, ...,\n",
       "         4.7856244e-01, 6.9382481e-02, 2.1778930e-02],\n",
       "        [6.4620497e-07, 3.8781318e-06, 1.2029222e-04, ...,\n",
       "         9.7424854e-06, 1.9348720e-06, 9.9980402e-01],\n",
       "        [5.5661218e-07, 3.5719877e-06, 8.2555916e-06, ...,\n",
       "         1.6066834e-06, 2.7906341e-07, 9.9997008e-01],\n",
       "        ...,\n",
       "        [4.5359948e-06, 5.9983868e-05, 2.2990735e-05, ...,\n",
       "         1.0650001e-05, 3.4846671e-06, 9.9926847e-01],\n",
       "        [4.4919375e-06, 5.9582810e-05, 2.2834231e-05, ...,\n",
       "         1.0587838e-05, 3.4508214e-06, 9.9927384e-01],\n",
       "        [4.5360002e-06, 6.0005910e-05, 2.2957109e-05, ...,\n",
       "         1.0660419e-05, 3.4914838e-06, 9.9926966e-01]],\n",
       "\n",
       "       [[1.2631206e-01, 5.5343550e-02, 3.0285068e-02, ...,\n",
       "         4.7856244e-01, 6.9382481e-02, 2.1778930e-02],\n",
       "        [6.4620497e-07, 3.8781318e-06, 1.2029222e-04, ...,\n",
       "         9.7424854e-06, 1.9348720e-06, 9.9980402e-01],\n",
       "        [5.5661218e-07, 3.5719877e-06, 8.2555916e-06, ...,\n",
       "         1.6066834e-06, 2.7906341e-07, 9.9997008e-01],\n",
       "        ...,\n",
       "        [4.5359948e-06, 5.9983868e-05, 2.2990735e-05, ...,\n",
       "         1.0650001e-05, 3.4846671e-06, 9.9926847e-01],\n",
       "        [4.4919375e-06, 5.9582810e-05, 2.2834231e-05, ...,\n",
       "         1.0587838e-05, 3.4508214e-06, 9.9927384e-01],\n",
       "        [4.5360002e-06, 6.0005910e-05, 2.2957109e-05, ...,\n",
       "         1.0660419e-05, 3.4914838e-06, 9.9926966e-01]],\n",
       "\n",
       "       [[1.2631206e-01, 5.5343550e-02, 3.0285068e-02, ...,\n",
       "         4.7856244e-01, 6.9382481e-02, 2.1778930e-02],\n",
       "        [6.4620497e-07, 3.8781318e-06, 1.2029222e-04, ...,\n",
       "         9.7424854e-06, 1.9348720e-06, 9.9980402e-01],\n",
       "        [5.5661218e-07, 3.5719877e-06, 8.2555916e-06, ...,\n",
       "         1.6066834e-06, 2.7906341e-07, 9.9997008e-01],\n",
       "        ...,\n",
       "        [4.5359948e-06, 5.9983868e-05, 2.2990735e-05, ...,\n",
       "         1.0650001e-05, 3.4846671e-06, 9.9926847e-01],\n",
       "        [4.4919375e-06, 5.9582810e-05, 2.2834231e-05, ...,\n",
       "         1.0587838e-05, 3.4508214e-06, 9.9927384e-01],\n",
       "        [4.5360002e-06, 6.0005910e-05, 2.2957109e-05, ...,\n",
       "         1.0660419e-05, 3.4914838e-06, 9.9926966e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = model.predict(test_ds,verbose=1)\n",
    "\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:52:05.652106Z",
     "iopub.status.busy": "2024-05-17T06:52:05.651207Z",
     "iopub.status.idle": "2024-05-17T06:52:05.657789Z",
     "shell.execute_reply": "2024-05-17T06:52:05.656883Z",
     "shell.execute_reply.started": "2024-05-17T06:52:05.652069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 2000, 13)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:52:23.273286Z",
     "iopub.status.busy": "2024-05-17T06:52:23.272297Z",
     "iopub.status.idle": "2024-05-17T06:52:23.281142Z",
     "shell.execute_reply": "2024-05-17T06:52:23.280253Z",
     "shell.execute_reply.started": "2024-05-17T06:52:23.273240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 12, 12, ..., 12, 12, 12],\n",
       "       [10,  2,  8, ..., 12, 12, 12],\n",
       "       [10, 12, 12, ..., 12, 12, 12],\n",
       "       ...,\n",
       "       [10, 12, 12, ..., 12, 12, 12],\n",
       "       [10, 12, 12, ..., 12, 12, 12],\n",
       "       [10, 12, 12, ..., 12, 12, 12]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取最后一个维度的预测结果\n",
    "test_preds = np.argmax(test_preds,axis=-1)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:53:45.202297Z",
     "iopub.status.busy": "2024-05-17T06:53:45.201539Z",
     "iopub.status.idle": "2024-05-17T06:53:45.207303Z",
     "shell.execute_reply": "2024-05-17T06:53:45.206347Z",
     "shell.execute_reply.started": "2024-05-17T06:53:45.202261Z"
    }
   },
   "outputs": [],
   "source": [
    "#移除增补的重复数据\n",
    "test_docs = test_docs[:-need_samples]\n",
    "test_token_ids = test_token_ids[:-need_samples]\n",
    "test_preds = test_preds[:-need_samples]\n",
    "test_words = test_words[:-need_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:56:10.122267Z",
     "iopub.status.busy": "2024-05-17T06:56:10.121314Z",
     "iopub.status.idle": "2024-05-17T06:56:10.151059Z",
     "shell.execute_reply": "2024-05-17T06:56:10.149931Z",
     "shell.execute_reply.started": "2024-05-17T06:56:10.122229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af60a15f0a8641b794e097bb8b7b5352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#掩码筛选数据\n",
    "document_list = []\n",
    "token_id_list = []\n",
    "label_id_list = []\n",
    "token_list = []\n",
    "\n",
    "for doc, token_ids, preds, tokens in tqdm(\n",
    "    zip(test_docs, test_token_ids, test_preds, test_words), total=len(test_words)\n",
    "):\n",
    "    # Create mask for filtering\n",
    "    #筛选非token 开始字符、预测有值、标记特殊字符的标签\n",
    "    mask1 = np.concatenate(([True], token_ids[1:] != token_ids[:-1])) # ignore non-start tokens of a word\n",
    "    mask2 = (preds != 12) # ignore `O` (BIO format) label -> 12 (integer format) label\n",
    "    mask3 = (token_ids != -1)  # ignore [CLS], [SEP], and [PAD] tokens\n",
    "    mask = (mask1 & mask2 & mask3) # merge filters\n",
    "    \n",
    "    # Apply filter\n",
    "    #进行筛选\n",
    "    token_ids = token_ids[mask]\n",
    "    preds = preds[mask]\n",
    "\n",
    "     # Store prediction if number of tokens is not zero\n",
    "    if len(token_ids):\n",
    "        token_list.extend(tokens[token_ids])\n",
    "        document_list.extend([doc] * len(token_ids))\n",
    "        token_id_list.extend(token_ids)\n",
    "        label_id_list.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:57:13.097255Z",
     "iopub.status.busy": "2024-05-17T06:57:13.096877Z",
     "iopub.status.idle": "2024-05-17T06:57:13.129683Z",
     "shell.execute_reply": "2024-05-17T06:57:13.128699Z",
     "shell.execute_reply.started": "2024-05-17T06:57:13.097228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label_id</th>\n",
       "      <th>token_string</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>2</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>8</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>2</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>742</td>\n",
       "      <td>8</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Diego</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Estrada</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>464</td>\n",
       "      <td>2</td>\n",
       "      <td>Diego</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>465</td>\n",
       "      <td>8</td>\n",
       "      <td>Estrada</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  document  token  label_id token_string           label\n",
       "0       0         7      9         2     Nathalie  B-NAME_STUDENT\n",
       "1       1         7     10         8        Sylla  I-NAME_STUDENT\n",
       "2       2         7    482         2     Nathalie  B-NAME_STUDENT\n",
       "3       3         7    483         8        Sylla  I-NAME_STUDENT\n",
       "4       4         7    741         2     Nathalie  B-NAME_STUDENT\n",
       "5       5         7    742         8        Sylla  I-NAME_STUDENT\n",
       "6       6        10      0         2        Diego  B-NAME_STUDENT\n",
       "7       7        10      1         8      Estrada  I-NAME_STUDENT\n",
       "8       8        10    464         2        Diego  B-NAME_STUDENT\n",
       "9       9        10    465         8      Estrada  I-NAME_STUDENT"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建表格\n",
    "pred_df = pd.DataFrame(\n",
    "    {\n",
    "        \"document\": document_list,\n",
    "        \"token\": token_id_list,\n",
    "        \"label_id\": label_id_list,\n",
    "        \"token_string\": token_list,\n",
    "    }\n",
    ")\n",
    "pred_df = pred_df.rename_axis(\"row_id\").reset_index() # add `row_id` column\n",
    "pred_df[\"label\"] = pred_df.label_id.map(CFG.id2label) # map integer label to BIO format label\n",
    "pred_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T06:58:19.866071Z",
     "iopub.status.busy": "2024-05-17T06:58:19.865638Z",
     "iopub.status.idle": "2024-05-17T06:58:19.878547Z",
     "shell.execute_reply": "2024-05-17T06:58:19.877578Z",
     "shell.execute_reply.started": "2024-05-17T06:58:19.866038Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df = pred_df.drop(columns=[\"token_string\", \"label_id\"]) # remove extra columns\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4308295,
     "sourceId": 7526248,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4442914,
     "sourceId": 7626226,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 4685,
     "sourceId": 6064,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
